{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python's Next Top Model\n",
    "By The Good, The Bad and the Ugly aka. The Three Musketeers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import itertools\n",
    "from ipywidgets import Layout\n",
    "import math\n",
    "\n",
    "import time\n",
    "from scipy import linalg\n",
    "import scipy.optimize as optimize\n",
    "import sympy as sm\n",
    "#from data_gen import gen_df \n",
    "from IPython.display import clear_output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome back to the wonderful Python universe of Credible Threats. We will continue the exploration of movies, and we're sure that our data project notebook will look at this notebook and think, *you complete me*. You might want to ask, *Magic mirror on the wall, who's the fairest one of all*, and the answer will be this notebook. This notebook will try to estimate the IMDb ratings of movies. And as a bonus, we even have a live-updating figure, which will leave you thinking, *it's alive, it's alive*. \n",
    "\n",
    "We will calculate the utility of each movie, given genres, awards, duration and so forth, and we will try to calculate the IMDb ratings for each movie, based on this utility. Finally, we will minimize the distance between our estimated ratings and the actual IMDb ratings. Now, we're sure you're thinking, *show me the money*, so this is it. \n",
    "\n",
    "Our utility function is given by: \n",
    "$$ U_i = \\sum_{k=1}^{23}(\\alpha_k G_{ik}) + \\sum_{k=1920}^{2010} (\\beta_k D_{ik}) + \\gamma N_i + \\delta W_i + \\rho L_i $$. \n",
    "\n",
    "$$\\text{Where, } G_{ik} \\text{ is genre } k \\text{, } D_{ik} \\text{ is decade } k \\text{, } N_i \\text{ is award nominations, } W_i \\text{ is award wins, and } L_i \\text{ is duration. } $$ \n",
    "\n",
    "I think we can agree, it is *beauty kills the beast*. \n",
    "Now, based on this utility function, we will estimate the ratings of each movies,\n",
    "$$ R_i^{model} = \\frac{\\exp(\\omega x_i')}{1 + \\exp(\\omega x_i')} $$ \n",
    "\n",
    "where, \n",
    "$$ x_i = \\big[G_1, G_2, ..., G_{23}, D_{1920}, D_{1930}, ..., D_{2010}, N_i, W_i, L_i \\big] $$\n",
    "$$ \\omega = \\big[\\alpha_1, \\alpha_2, ..., \\alpha_{23}, \\beta_{1920}, \\beta_{1930}, ..., \\beta_{2010}, \\gamma, \\delta, \\rho_1 \\big] $$\n",
    "\n",
    "We then use optimize methods to solve the following: \n",
    "$$ \\min_{\\omega} \\Big\\{ \\sum_{i=1}^{n} \\left( R_i^{model} - R_i^{data} \\right)^2 \\Big\\} $$\n",
    "\n",
    "$$ \\text{where, } R_i^{data} \\text{ are the true IMDb ratings from the dataset, and} R_i^{model} \\text{are the ratings calculated from the utility function.}$$\n",
    "\n",
    "Throughout our notebook, you might *feel the need - the need for speed*. But, *patience you must have, my young Padawan*, because optimizing takes time. \n",
    "\n",
    "*Of all the python notebooks in all the towns in all the world, you walk into ours*. How lucky you are, you'll soon see why. \n",
    "\n",
    "So, *say hello to my little friend*, Python's next top model.  \n",
    "Let's *get busy coding, or get busy dying*. \n",
    "\n",
    "![test](https://media.giphy.com/media/LpkBAUDg53FI8xLmg1/giphy.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for later use \n",
    "variables = ['Action','Adventure','Animation','Biography','Comedy','Crime','Documentary',\n",
    "            'Drama','Family','Fantasy','FilmNoir','History','Horror','Music','Musical',\n",
    "            'Mystery','Romance','SciFi','Short','Sport','Thriller','War','Western',\n",
    "            'decade_1920','decade_1930','decade_1940','decade_1950','decade_1960',\n",
    "            'decade_1970','decade_1980','decade_1990','decade_2000','decade_2010',\n",
    "            'nrOfNominations','nrOfWins','duration']\n",
    "\n",
    "vars_dec = ['Action','Adventure','Animation','Biography','Comedy','Crime','Documentary',\n",
    "        'Drama','Family','Fantasy','FilmNoir','History','Horror','Music','Musical',\n",
    "        'Mystery','Romance','SciFi','Short','Sport','Thriller','War','Western',\n",
    "        'nrOfNominations','nrOfWins','duration']\n",
    "\n",
    "decade_list = [1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010]\n",
    "\n",
    "\n",
    "# Function to calculate squared mean differences \n",
    "def sqr_diff_sum(df_X,pars):\n",
    "    \"\"\" Generates a float of the sum of squared differences between the ratings from the data and the model. \n",
    "\n",
    "    Args: \n",
    "        df_X (DataFrame): DataFrame containing the variable in x for all observations.\n",
    "        pars (List): List of parameters in omega\n",
    "\n",
    "    Returns: \n",
    "        A float.\n",
    "    \"\"\"\n",
    "    # Calculate the matrix product between omega and X\n",
    "    util = df_X@pars  \n",
    "    # Scale the product so it's between 0 and 10. This is the R_model\n",
    "    df_Y['rat_model'] = 10*np.exp(util)/(1+np.exp(util)) \n",
    "    # Calculate the squared difference between R_data and R_model\n",
    "    df_Y['sqr_diff'] = (df_Y['rat_model']-df_Y['rat_data'])**2 \n",
    "\n",
    "    return df_Y['sqr_diff'].sum() # Returns the sum of the squared differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_df(filename):\n",
    "\n",
    "    # Get .csv.file\n",
    "    data = pd.read_csv(filename, sep=';', encoding='latin-1', escapechar='\\\\')\n",
    "\n",
    "    # Read file into pandas dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Drop unwanted columns\n",
    "    for i in range(44,48):\n",
    "        df.drop(columns=[f'Unnamed: {i}'], inplace=True)\n",
    "\n",
    "    # Filters out movies with less than 5000 ratings and drops some genres and other columns\n",
    "    df = df.loc[(df['ratingCount']>=5000)]\n",
    "    df.drop(columns=['fn','wordsInTitle','url','Adult','GameShow','News','RealityTV','TalkShow'], inplace=True)\n",
    "\n",
    "    # Keep only observations of movie-type\n",
    "    I = df['type'] == 'video.movie'\n",
    "    df = df.loc[I]\n",
    "    df.drop(columns=['type'], inplace=True)\n",
    "\n",
    "    # Drop observations with missing data\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Replace 0's in imdb-ratings\n",
    "    df['imdbRating'] = df['imdbRating'].astype(str)\n",
    "    df['imdbRating'].replace(regex=True, inplace=True,to_replace='0',value='')\n",
    "    df['imdbRating'] = df['imdbRating'].astype(float)\n",
    "\n",
    "    # Transform duration from seconds to hours\n",
    "    df['duration'] = df['duration']/60**2\n",
    "\n",
    "    # Drop years before 1920 and 2014 because of few obervations\n",
    "    I = (df['year']>=1920) & (df['year']<=2013)\n",
    "    df = df.loc[I]\n",
    "\n",
    "    # Change the type of 'year' to integer\n",
    "    df['year'] = df['year'].astype(int)    \n",
    "    \n",
    "    # Sort observations and reset index\n",
    "    df.sort_values('year', inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "    # Generating variable that shows the decade (as a string)\n",
    "    year_list = [1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010]\n",
    "\n",
    "    df['decade'] = ''\n",
    "    for i,start in enumerate(year_list):\n",
    "        end = start+10\n",
    "        df.loc[(df['year'] >= start) & (df['year'] < end), 'decade'] = f'{year_list[i]}s'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_s(keep_top=None, decade=None):\n",
    "    \"\"\" Prepares data for optimization by creating dummmy-variables for decades, dropping movies with less than 5000 ratings,\n",
    "        dropping some variables that we don't wish to use in the model, and splitting the dataset, so we have a data set\n",
    "        consisting only of the variables used in the model.\n",
    "        \n",
    "    Args:\n",
    "        keep_top (optional, type: int): When a number (n) is put in this option, only the top n movies a kept in the dataset. This is used to remove movies for a better fit of the model\n",
    "\n",
    "    Returns:\n",
    "        df_X (type: Pandas dataframe): A dataframe consisting only of the varaibles that are used to calculate the rating.\n",
    "        df_Y (type: Pandas dataframe): A dataframe consisting of the true ratings from the dataset and an index-variable, used for merging with original dataset, later in the process.\n",
    "\n",
    "    Notice:\n",
    "        The function includes global for 'df', which means that the dataframe 'df' generated, can be called outside the function. This is used so that we won't have to filter the original dataframe everytime we use it.\n",
    "    \"\"\"\n",
    "    global df\n",
    "\n",
    "    # Calls the file 'imdb.csv' which is located in the repository, and contains the dataset used\n",
    "    filename = 'imdb.csv'\n",
    "\n",
    "    # A function generated earlier, which cleans the dataset [Should we include this, and do some of the filtering below in that funciton?]\n",
    "    df = gen_df(filename)\n",
    "            \n",
    "    # Keeps only top n movies, if this is specified when calling the function\n",
    "    if keep_top != None:\n",
    "        df = df.sort_values('imdbRating', ascending=False)\n",
    "        df = df.iloc[:keep_top]\n",
    "    \n",
    "    if decade == None:\n",
    "        # Decade dummies\n",
    "        for i in decade_list:\n",
    "            df[f'decade_{i}'] = 0\n",
    "            df.loc[(df['decade'] == f'{i}s'),f'decade_{i}'] = 1\n",
    "        \n",
    "        # Splits the dataset into two datasets\n",
    "        df_X = df.copy()\n",
    "        df_Y = pd.DataFrame(df[['imdbRating', 'index']].copy())\n",
    "        df_Y = df_Y.rename(columns = {'imdbRating':'rat_data'})\n",
    "\n",
    "        # Rearrange and keep given columns \n",
    "        df_X = df_X.reindex(['Action','Adventure','Animation','Biography','Comedy','Crime','Documentary',\n",
    "                              'Drama','Family','Fantasy','FilmNoir','History','Horror','Music','Musical',\n",
    "                              'Mystery','Romance','SciFi','Short','Sport','Thriller','War','Western',\n",
    "                              'decade_1920','decade_1930','decade_1940','decade_1950','decade_1960',\n",
    "                              'decade_1970','decade_1980','decade_1990','decade_2000','decade_2010',\n",
    "                              'nrOfNominations','nrOfWins','duration'], axis=1)\n",
    "            \n",
    "    \n",
    "    if decade != None:\n",
    "        # Keeps movies from the specified decade\n",
    "        df = df.loc[df['decade'] == f'{decade}s'] \n",
    "\n",
    "        # Splits the dataset into two datasets\n",
    "        df_X = df.copy()\n",
    "        df_Y = pd.DataFrame(df[['imdbRating', 'index']].copy())\n",
    "        df_Y = df_Y.rename(columns = {'imdbRating':'rat_data'})\n",
    "\n",
    "        # Rearrange columns and keep the specified variables \n",
    "        df_X = df_X.reindex(['Action','Adventure','Animation','Biography','Comedy','Crime','Documentary',\n",
    "                                  'Drama','Family','Fantasy','FilmNoir','History','Horror','Music','Musical',\n",
    "                                  'Mystery','Romance','SciFi','Short','Sport','Thriller','War','Western',\n",
    "                                  'nrOfNominations','nrOfWins','duration'], axis=1)\n",
    "\n",
    "\n",
    "    return df_X, df_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "We analyze preferences for movies by minimizing the sum of the squared differences from our model prediction and the true rating, i.e.:\n",
    "$$ \n",
    "\\min_{\\omega} \\Big\\{ \\sum_{i=1}^{n} \\left( R_i^{model} - R_i^{data} \\right)^2 \\Big\\} \n",
    "$$ \n",
    "By doing so, we'll find parameters that best describe movie preferences given our model. As we will discuss later, our model is not necessarily the best model to describe preferences, which might lead to these preferences not being accurate. <br>\n",
    "If the model could predict all movie ratings correctly the parameters would perfectly describe movie preferences. For example if the parameter for action-movies were negative, the prediction would be precise, if action-movies generally had a negative impact on ratings. Of course people have much different preferences, and an interesting analysis would be, to predict preferences for different people, and by this look at a distribution of these parameters. This is not included in this project, though, mainly because of we don't have the data for it. <br>\n",
    "To shorten the length of the optimizing process, and to sort out movies that makes it harder for the model to predict preferences, the optimization can be based on the top n movies, based on IMDb rating. When all movies are included in the optimizer the lowest ranking movies will have ratings around 2. Our rating predictor will in this case not rank any movies under 5. Thereby it is clear that the predictor model is not able to predict movies with very low rating. But it is able to predict movies with higher ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(keep_top=None, live_graph=True):\n",
    "    \"\"\" Creates and optimizes the function which calculates rating based on variables in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        keep_top (optional, type: int): When a number (n) is put in this option, only the top n movies a kept in the dataset. This is used to sort out movies for a better fit of the model\n",
    "        live_graph(optional, type: boolean): Produces a live graph of the optimization proces, plotting the function values against the iterations. Makes the optimizer slower, but is a good visualization of the process.\n",
    "                                             \n",
    "    Returns:\n",
    "        result (type: scipy optimizer object): An object containing results from the optimizer among other information such as status etc.\n",
    "        timer (type: int): Time of running optimizer in seconds.\n",
    "                                               \n",
    "    Notice:\n",
    "        Uses the package scipy.optimize.minimize which optimizes functions based on a given method. Here 'Nelder-Mead' is used.\n",
    "        This function defines a set of functions used in the optimizer which are not documented by docstring but comments in code.\n",
    "    \"\"\"\n",
    "    # \"Initializes\" a timer for printing time of optimization\n",
    "    start = time.time()\n",
    "    \n",
    "    # Set of globals, so variables can be called inside the functions defined in this function\n",
    "    global fs\n",
    "    global evals\n",
    "    global x0\n",
    "    global df\n",
    "    global df_Y\n",
    "        \n",
    "    # Plots a live graph of optimization process, if chosen.\n",
    "    if live_graph:\n",
    "        # The live plot is created by taking the function value for each iteration and saving in a list\n",
    "        def live_plot(evals, fs, ymax=10000, figsize=(7,5)):\n",
    "            # Clears plot every time a new plot is created\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "            # Plots values and sets title, grid, labels, etc.\n",
    "            plt.plot(evals, fs)\n",
    "            plt.title('Figure 1: Optimizing path')\n",
    "            plt.grid(True)\n",
    "            plt.xlabel('Iterations')\n",
    "            plt.ylabel('Function value')\n",
    "            plt.xlim(0,17500)\n",
    "            plt.ylim(0,ymax)\n",
    "\n",
    "            plt.show();\n",
    "\n",
    "    # Function to be called by optimizer each iteration. This returns function values for each iteration and plots live graph\n",
    "    def collect(x):\n",
    "        # Set of globals to be called in live graph-function\n",
    "        global fs\n",
    "        global evals\n",
    "        global x0\n",
    "        global fig\n",
    "        global ax\n",
    "        global ymax\n",
    "        global x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12, x_13, x_14, x_15, x_16, x_17, x_18, x_19, x_20\n",
    "        global x_21, x_22, x_23, x_24, x_25, x_26, x_27, x_28, x_29, x_30, x_31, x_32, x_33, x_34, x_35, x_36\n",
    "\n",
    "\n",
    "        # Calculates function value for initial guess, for first iteration\n",
    "        if evals == 0:\n",
    "            fs = []\n",
    "            fs.append(obj_fun(x0))\n",
    "            # Generates vector of parameter estimates\n",
    "            for i in range(1,37): # 1 to 36\n",
    "                globals()[f'x_{i}'] = [x0[i-1]]\n",
    "\n",
    "        # Calculates function values for current parameter values\n",
    "        if evals != 0:\n",
    "            fs.append(obj_fun(x))\n",
    "        \n",
    "        # Plots live graph if chosen\n",
    "        if live_graph:\n",
    "            # Calculates max y-value for axis for first iteration\n",
    "            if evals == 1:\n",
    "                ymax = math.ceil(obj_fun(x)/2000)*2000\n",
    "\n",
    "            # Updates plot every 100th iteration for the first 1000 iterations\n",
    "            if evals < 1000:\n",
    "                if evals > 0 and evals%100 == 0:\n",
    "                    live_plot(range(evals+1), fs, ymax)\n",
    "            # Updates plot every 500th iteration for the rest of the process\n",
    "            if evals >=1000:\n",
    "                if evals%500 == 0:\n",
    "                    live_plot(range(evals+1), fs, ymax)\n",
    "        \n",
    "        # Appends esimtates to x_vectors \n",
    "        for i in range(1,37):\n",
    "            globals()[f'x_{i}'].append(x[i-1])\n",
    "\n",
    "        # Adds one to the number of iterations each iteration\n",
    "        evals += 1\n",
    "    \n",
    "    # Define datasets to be used using function defined earlier\n",
    "    df_X, df_Y = df_s(keep_top=keep_top, decade=None)\n",
    "    \n",
    "    # Intiial guess\n",
    "    x0 = np.zeros(len(variables))\n",
    "    \n",
    "    # Start iteration number\n",
    "    evals = 0\n",
    "    \n",
    "    # Defines function to be minimized\n",
    "    obj_fun = lambda x: sqr_diff_sum(df_X,x)\n",
    "    \n",
    "    # Run optimizer\n",
    "    result = optimize.minimize(obj_fun,x0,\n",
    "                               method=\"Nelder-Mead\",\n",
    "                               options={\"disp\":True, \"maxiter\":50000}, # display the results\n",
    "                               callback=collect\n",
    "                               ) \n",
    "    \n",
    "    # End timer\n",
    "    end = time.time()-start\n",
    "    \n",
    "    # Returns\n",
    "    return result, end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By runing the code below you can see a live-updating graph that shows the value of the sum of the squared difference for each iteration. As you will see, the sqaured difference reaches close to its minimum very quickly so the last 10,000 iterations almost don't change the value of the squard difference. The live-graph slows the optimizer down, so choose option live_graph=False, if you are impacient and don't care about an awesome live-updating graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3735.382286\n",
      "         Iterations: 16566\n",
      "         Function evaluations: 19203\n",
      "         Time: 118.4200 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run optimizer function and print results and time\n",
    "result, timer = optimizer()\n",
    "print(f'{\" \":9s}Time: {timer:.4f} seconds')\n",
    "\n",
    "# Keep copy of df and df_Y for later purpose\n",
    "df_Y_all = df_Y.copy()\n",
    "df_all = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will give a list of the estimated parameters/preferences, and the cell after that produce a figure showing the path of the estimated parameters, i.e. the parameter estimate for each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : -0.02\n",
      "Adventure : -0.03\n",
      "Animation : 0.17\n",
      "Biography : -0.17\n",
      "Comedy : -0.02\n",
      "Crime : -0.03\n",
      "Documentary : 0.11\n",
      "Drama : 0.13\n",
      "Family : -0.06\n",
      "Fantasy : -0.33\n",
      "FilmNoir : 0.02\n",
      "History : -0.08\n",
      "Horror : -0.02\n",
      "Music : -0.03\n",
      "Musical : -0.08\n",
      "Mystery : -0.09\n",
      "Romance : -0.04\n",
      "SciFi : -0.10\n",
      "Short : 0.12\n",
      "Sport : -0.06\n",
      "Thriller : -0.06\n",
      "War : 0.19\n",
      "Western : 0.03\n",
      "decade_1920 : -0.04\n",
      "decade_1930 : 0.12\n",
      "decade_1940 : 0.18\n",
      "decade_1950 : -0.03\n",
      "decade_1960 : 0.31\n",
      "decade_1970 : 0.10\n",
      "decade_1980 : 0.01\n",
      "decade_1990 : -0.22\n",
      "decade_2000 : -0.20\n",
      "decade_2010 : -0.21\n",
      "nrOfNominations : 0.00\n",
      "nrOfWins : 0.02\n",
      "duration : 0.45\n"
     ]
    }
   ],
   "source": [
    "# List of Parameter Estimates \n",
    "\n",
    "# Constructs dictionary with parameters \n",
    "preben = dict()\n",
    "for j,i in enumerate(variables):\n",
    "    preben[i] = f'{result.x[j]:.2f}'\n",
    "\n",
    "\n",
    "# Prints list of parameterestimes \n",
    "for i in preben: \n",
    "    print (i, \":\", preben[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b463ab8286b4e6ea1a98148a3de78e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Variable', options=('Action', 'Adventure', 'Animation', 'Biography…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Convergence of Parameter Estimates \n",
    "\n",
    "# Constructing dataframe which contains all parameter vectors as variables\n",
    "par_dict = {\"Action\": x_1,\n",
    "             \"Adventure\": x_2,\n",
    "             \"Animation\": x_3,\n",
    "             \"Biography\": x_4,\n",
    "             \"Comedy\": x_5,\n",
    "             \"Crime\": x_6,\n",
    "             \"Documentary\": x_7,\n",
    "             \"Drama\": x_8, \n",
    "             \"Family\": x_9,\n",
    "             \"Fantasy\": x_10,\n",
    "             \"Film Noir\": x_11,\n",
    "             \"History\": x_12,\n",
    "             \"Horror\": x_13,\n",
    "             \"Music\": x_14,\n",
    "             \"Musical\": x_15,\n",
    "             \"Mystery\": x_16,\n",
    "             \"Romance\": x_17,\n",
    "             \"SciFi\": x_18, \n",
    "             \"Short\": x_19,\n",
    "             \"Sport\": x_20,\n",
    "             \"Thriller\": x_21,\n",
    "             \"War\": x_22,\n",
    "             \"Western\": x_23,\n",
    "             \"1920's\": x_24,\n",
    "             \"1930's\": x_25,\n",
    "             \"1940's\": x_26,\n",
    "             \"1950's\": x_27,\n",
    "             \"1960's\": x_28,\n",
    "             \"1970's\": x_29,\n",
    "             \"1980's\": x_30,\n",
    "             \"1990's\": x_31,\n",
    "             \"2000's\": x_32,\n",
    "             \"2010's\": x_33,\n",
    "             \"Nr. of Nominations\": x_34, \n",
    "             \"Nr. of Wins\": x_35, \n",
    "             \"Duration\": x_36\n",
    "            }\n",
    "\n",
    "df_par = pd.DataFrame(par_dict)\n",
    "\n",
    "\n",
    "##  Interactive graph of parametervalues for each iteration in optimizer \n",
    "# Chose which parameter to show \n",
    "\n",
    "# Interactive graph of parameter estimates\n",
    "def graph(par):\n",
    "    \"\"\"\n",
    "    Constructs a graph which shows the progress in parameter estimates foreach iteration in\n",
    "    the optimizing process. \n",
    "    \n",
    "    Args: \n",
    "        par (type: str): Variable name, for which the graphs shows the parameter estimates. \n",
    "                \n",
    "    Returns: \n",
    "        Graph of progress in parameter estimates. \n",
    "        \n",
    "    Notice: \n",
    "        The function is meant to be called within the graph_int function. \n",
    "        The graph_int function turns this graph into an interactive function, where the \n",
    "        user can choose between variables. \n",
    "        Thus, the graph is not meant to be run on its own. \n",
    "    \"\"\"\n",
    "    # Generating figure \n",
    "    plt.plot(df_par[par])\n",
    "    \n",
    "    # Grid and axes \n",
    "    plt.grid(True)\n",
    "    plt.axhline(df_par[par].iloc[-1], linestyle=\"dashed\", color=\"orange\", label=\"Optimum\")\n",
    "    \n",
    "    # Labels and Titles \n",
    "    plt.title(\"Figure 2: Progress in Parameter Estimation\")\n",
    "    plt.ylabel(\"Parameter Estimate\")\n",
    "    plt.xlabel(\"Nr. of Iterations\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "# Interactive part: Choose between parameters \n",
    "def graph_int(): \n",
    "    \"\"\"\n",
    "    Constructs interactive graph from the graph function. \n",
    "    \"\"\"\n",
    "    widgets.interact(graph, \n",
    "                     par = widgets.Dropdown( \n",
    "                     description=\"Variable\", \n",
    "                     options=df_par.columns.values, \n",
    "                     value=\"Action\"));\n",
    "graph_int()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure, we notice that most parameter estimates are very volatile in the first 10,000-15,000 iterations, following which they converge towards the optimum value. \n",
    "\n",
    "However, this does not appear to be the case for the variables, Nr. of nominations and Nr. of awards, which get quite close to the optimum value at around the 7500th iteration. \n",
    "Additionally, we notice that the variable Duration appears to have the largest effect on ratings, which the optimzer seems to realize after approximately 2500 iterations. \n",
    "\n",
    "In the next part of the project we will plot the optimal estimates in a figure. Furthermore, we show how the estimates changes a lot when the sample size is restricted to only the best movies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 197.838633\n",
      "         Iterations: 13628\n",
      "         Function evaluations: 15779\n",
      "         Time: 63.0919 seconds\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 439.055453\n",
      "         Iterations: 21224\n",
      "         Function evaluations: 24448\n",
      "         Time: 94.0714 seconds\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 903.321786\n",
      "         Iterations: 16176\n",
      "         Function evaluations: 18626\n",
      "         Time: 78.6587 seconds\n"
     ]
    }
   ],
   "source": [
    "## This piece of code solves the model for three different sample size\n",
    "\n",
    "#### OBS: The code will take a couple of minutes to run because the model is solved three times!! ####\n",
    "\n",
    "# List to store result for the optimization with the different sample sizes\n",
    "results_list = [result.x]\n",
    "\n",
    "# This loop solve the model for the top 500, 1000, and 2000 movies\n",
    "for i in [500, 1000, 2000]:\n",
    "    res_temp, timer_temp = optimizer(keep_top=i, live_graph=False)\n",
    "    temp = res_temp.x\n",
    "    print(f'{\" \":9s}Time: {timer_temp:.4f} seconds')\n",
    "    results_list.append(temp) # Store the results in the 'results_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942b29c083864672adc9be8d633dc4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Nr. of movies', options=('All', 500, 1000, 2000), value='All'), Ou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## The code creates an interactive plot of the estimated parameters for all variables. \n",
    "## In the interactive part you can choose between estimates when the model is solved \n",
    "## for all movies or just top top 500, 1000, or 2000 rated movies. \n",
    "\n",
    "def fig_2(val):\n",
    "    \"\"\" Generates a figure which plots estimated parameters for all variables.\n",
    "\n",
    "    Args: \n",
    "        val (string or int): Should be one of the elements in the options-list\n",
    "\n",
    "    Returns: \n",
    "        One interactive plot.  \n",
    "\n",
    "    Notice: \n",
    "        The function is generated so that it can be called using widgets.interact. \n",
    "        Thus, it is not intended to be used on its own. \n",
    "    \"\"\"   \n",
    "    # Initiates figure\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    ax1 = fig.add_subplot(1,1,1)\n",
    "    \n",
    "    # Plots the estimated parameters for the chosen sample of movies\n",
    "    ax1.bar(variables, results_list[options.index(val)], label=f'Estimates with {options[options.index(val)]} movies')\n",
    "    \n",
    "    # Scatter plot with the estimated paramters for the entire sample \n",
    "    ax1.scatter(variables, results_list[0], marker='D', s=15, zorder=2, label='Estimates with all movies')\n",
    "    \n",
    "    # Legends and labels \n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_ylabel('Parameter estimates')\n",
    "    ax1.set_title(f'Figure 3: Parameter estimates for {options[options.index(val)]} movies')\n",
    "    ax1.set_ylim([-0.7,0.7])\n",
    "    ax1.axhline(y=0,color='black',linewidth=1)\n",
    "    for tick in ax1.get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "        \n",
    "options = ['All', 500, 1000, 2000] # Option list\n",
    "\n",
    "# Making the figure interactive so the estimates are shown for the chosen sample size \n",
    "widgets.interact(fig_2,\n",
    "    val = widgets.Dropdown(description='Nr. of movies', value='All', options=options, \n",
    "                ),\n",
    ");   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As also showed in the list above, duration is the varaible that effects the ratings the most. This is also the case when the model is solved for top 500, 1000, and 2000 rated movies, cf. figure 3. However, it seems like duration is the only estimate which is fairly stable. The sign of the decade dummies is an example. When the model is solved for the entire sample, the estimates are positive for the older decade-dummies, i.e. the 1920s and 1930s, and negative for the newer decade-dummies, the 2000s and 2010s. However, when the sample only contains the top 2000 rated movies we see the opposite picture. We know from figure 10 in our dataproject that the ratings of the younger movies have a larger variance with more high-rated movies but also a lot of poorly rated movies. By restricting the sample to only the best movies we drop all the bad new movies, and this is probably the reason to the positive estimates of the newer movies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction precision\n",
    "Ideally our model would predict movie ratings perfectly, and thereby our parameters would perfectly describe the general preferences for movies. Whether this is possible, even with an advanced model, is not sure. People have different preferences, and two movies that would seem identical in our dataset, based on genres, duration, time of release, etc. could have very different ratings. Therefore some devitaion in our prediction from the true ratings are expected. For that reason, this section will explore how well our model describe the actual ratings, and why some estimates are more reliable than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493a1b1708bb4e099291db20c093a933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Group', options=('Genres', 'Decades', 'Duration', 'True rating'), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def merge_df(df_org):\n",
    "    \"\"\" Merges the original dataset with the optimal solution from the optimizer, i.e. the last dataset of calculated raitngs.\n",
    "    \n",
    "    Args:\n",
    "        df_org (type: Pandas dataframe): The orignal dataset of movies, containing true ratings.\n",
    "        \n",
    "    Returns:\n",
    "        df_merge (type: Pandas dataframe): Original dataset combined with calculated ratings from optimal parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Merges original dataset with optimal ratings based on model\n",
    "    df_merge = df_org.merge(df_Y_all, how='right', left_on='index', right_on='index')\n",
    "\n",
    "    # Calculates deviations both in normal and absolute form \n",
    "    df_merge['abs_diff'] = abs(df_merge['rat_model']-df_merge['rat_data'])\n",
    "    df_merge['diff'] = df_merge['rat_model']-df_merge['rat_data']\n",
    "    \n",
    "    return df_merge\n",
    "\n",
    "\n",
    "def _mean_genre(df,group, diff):  \n",
    "    \"\"\" Calculates the mean of deviations from true ratings and ratings based on optimal parameters from optimizer.\n",
    "    \n",
    "    Args:\n",
    "        df (type: Pandas dataframe): Dataframe consisting information on groups and deivations\n",
    "        group (type: string): Defines which group mean of deviations are presented for. Chosen by fixed list in widget.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "        \n",
    "    Notice:\n",
    "        Only to be run through widget.interact()\n",
    "    \"\"\"\n",
    "    # List of values used for each group\n",
    "    genre_list = ['Action','Adventure','Animation','Biography','Comedy','Crime','Documentary',\n",
    "                    'Drama','Family','Fantasy','FilmNoir','History','Horror','Music','Musical',\n",
    "                    'Mystery','Romance','SciFi','Short','Sport','Thriller','War','Western']\n",
    "    \n",
    "    decade_list = ['1920s', '1930s', '1940s', '1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
    "    \n",
    "    dur_list = [0, .5, 1, 1.5, 2, 2.5, 3, 3.5]\n",
    "    \n",
    "    rat_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    \n",
    "    # Dictionaries for plotting\n",
    "    mean_dict = dict()\n",
    "    count_dict = dict()\n",
    "    \n",
    "    # Choose which type of difference to plot\n",
    "    if diff == 'Absolute':\n",
    "        diff_col = 'abs_diff'\n",
    "    else:\n",
    "        diff_col = 'diff'\n",
    "    \n",
    "    # Creates dictionaries based on chosen group\n",
    "    if group == 'Genres':\n",
    "        for i in genre_list:\n",
    "            I = df[i] == 1\n",
    "            mean_dict[i] = df.loc[I][diff_col].mean()\n",
    "            count_dict[i] = df.loc[I]['index'].count()    \n",
    "\n",
    "    if group == 'Decades':\n",
    "        for i in decade_list:\n",
    "            I = df['decade'] == i\n",
    "            mean_dict[i] = df.loc[I][diff_col].mean()\n",
    "            count_dict[i] = df.loc[I]['index'].count()\n",
    "\n",
    "    if group == 'Duration':\n",
    "        for j,i in enumerate(dur_list):\n",
    "            if j != len(dur_list)-1:\n",
    "                # Uses values from dict, to find movies between to items in the lists.\n",
    "                I = ((df['duration'] >= dur_list[j]) & (df['duration'] < dur_list[j+1]))\n",
    "                mean_dict[f'{dur_list[j]:2.1f} - {dur_list[j+1]:2.1f}'] = df.loc[I][diff_col].mean()\n",
    "                count_dict[f'{dur_list[j]:2.1f} - {dur_list[j+1]:2.1f}'] = df.loc[I]['index'].count()\n",
    "            else:\n",
    "                I = df['duration'] >= dur_list[j]\n",
    "                mean_dict[f'{dur_list[j]:2.1f} {\"+\":5s}'] = df.loc[I][diff_col].mean()\n",
    "                count_dict[f'{dur_list[j]:2.1f} {\"+\":5s}'] = df.loc[I]['index'].count()\n",
    "\n",
    "    if group == 'True rating':\n",
    "        for j,i in enumerate(rat_list):\n",
    "            if j != len(rat_list)-1:\n",
    "                # Uses values from dict, to find movies between to items in the lists.\n",
    "                I = ((df['rat_data'] >= rat_list[j]) & (df['rat_data'] < rat_list[j+1]))\n",
    "                mean_dict[f'{rat_list[j]:4.1f} - {rat_list[j+1]:3.1f}'] = df.loc[I][diff_col].mean()\n",
    "                count_dict[f'{rat_list[j]:4.1f} - {rat_list[j+1]:3.1f}'] = df.loc[I]['index'].count()\n",
    "            else:\n",
    "                pass\n",
    "                           \n",
    "    # Creates figure to hold two subplots \n",
    "    fig1, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,sharey=True,figsize=(12,5))\n",
    "    #plt.title(\"Figure 4\")\n",
    "                           \n",
    "    # Plots means of deviations\n",
    "    ax1.barh(*zip(*mean_dict.items()))\n",
    "    ax1.set_ylabel(group)\n",
    "    ax1.set_xlabel('Model deviation')\n",
    "    ax1.grid(axis='x')\n",
    "    ax1.set_title('Figure 4: Deviation between the model and the actual ratings')\n",
    "                           \n",
    "    # Plots count of movies\n",
    "    ax2.barh(*zip(*count_dict.items()))\n",
    "    ax2.set_xlabel('Number of observations')\n",
    "    ax2.grid(axis='x')\n",
    "    ax2.set_title('Figure 5: Number of observation')\n",
    "\n",
    "\n",
    "    \n",
    "# Calls merge-function\n",
    "df_merge = merge_df(df_all) \n",
    "\n",
    "# Runs interactive figure \n",
    "mean_genre = widgets.interact(_mean_genre, \n",
    "                             df = widgets.fixed(df_merge),\n",
    "                             group = widgets.Dropdown(\n",
    "                                 options = ['Genres','Decades','Duration','True rating'],\n",
    "                                 description = 'Group',\n",
    "                                 value = 'Genres'),\n",
    "                             diff = widgets.RadioButtons(\n",
    "                                 options = ['Average', 'Absolute'],\n",
    "                                 description = 'Difference',\n",
    "                                 value = 'Average'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4 plot the mean of deviations based on genres, duration, decades, and true ratings. Figure 5 plot the number of movies in the dataset based on the same groups. From these graphs we see a clear correlation between precision and number of movies in the dataset. The more movies of a given genre, duration, etc. the more precise the prediction of movie-ratings in these groups. Take drama-movies as an example. The mean of the deviation is close to zero, and movies of this genre is also quite overrepresentated in the dataset. The complete opposite case is short-movies. But we also see that sci-fi-movies are quite well predicted, even though there aren't relatively many of these movies in the dataset. The same is seen in other different genres. This general image is seen when we group movies on decades, duration, and true rating as well. <br>\n",
    "When looking at absolute differences instead, the correlation is not as strong, especially not for the genres.\n",
    "\n",
    "The next section will look at the changes in the estimates when the model is solved for one decade at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opmization 1 out of 10: 1920\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2.582968\n",
      "         Iterations: 11001\n",
      "         Function evaluations: 13220\n",
      "         Time: 26.7000 seconds\n",
      "Opmization 2 out of 10: 1930\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 15.556532\n",
      "         Iterations: 11136\n",
      "         Function evaluations: 13352\n",
      "         Time: 27.1140 seconds\n",
      "Opmization 3 out of 10: 1940\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 26.531688\n",
      "         Iterations: 8876\n",
      "         Function evaluations: 10731\n",
      "         Time: 21.1378 seconds\n",
      "Opmization 4 out of 10: 1950\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 51.634688\n",
      "         Iterations: 9578\n",
      "         Function evaluations: 11517\n",
      "         Time: 24.1116 seconds\n",
      "Opmization 5 out of 10: 1960\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 87.282326\n",
      "         Iterations: 13058\n",
      "         Function evaluations: 15676\n",
      "         Time: 31.4363 seconds\n",
      "Opmization 6 out of 10: 1970\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 43.500430\n",
      "         Iterations: 9229\n",
      "         Function evaluations: 11116\n",
      "         Time: 23.0775 seconds\n",
      "Opmization 7 out of 10: 1980\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 77.604277\n",
      "         Iterations: 8448\n",
      "         Function evaluations: 10155\n",
      "         Time: 24.3106 seconds\n",
      "Opmization 8 out of 10: 1990\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 88.532631\n",
      "         Iterations: 7482\n",
      "         Function evaluations: 9030\n",
      "         Time: 20.4512 seconds\n",
      "Opmization 9 out of 10: 2000\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 183.284443\n",
      "         Iterations: 9095\n",
      "         Function evaluations: 10933\n",
      "         Time: 25.4300 seconds\n",
      "Opmization 10 out of 10: 2010\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 44.839180\n",
      "         Iterations: 8593\n",
      "         Function evaluations: 10363\n",
      "         Time: 25.5514 seconds\n"
     ]
    }
   ],
   "source": [
    "## The following code produces the parameter estimates where the sample is restricted to all the decades,\n",
    "## one at the time. The function return a list which contains a list of the estimates for every decade.\n",
    "\n",
    "#### OBS: The code will take about 3-4 minutes to run, since the model is estimated 10 times!! ####\n",
    "\n",
    "def optimizer_dec():\n",
    "    \"\"\" Generates a list containing 10 list with estimates of the model for every decade. \n",
    "    \n",
    "    Args: \n",
    "        No arguments are needed.\n",
    "        \n",
    "    Returns: \n",
    "        A list.\n",
    "        \n",
    "    Notice: \n",
    "        It will raise an error if the function is given an argument!\n",
    "    \"\"\"\n",
    "              \n",
    "    result = [] # Empty list to store the estimated parameters\n",
    "    \n",
    "    # Make a loop so the model is estimated for all ten decades\n",
    "    for j,decade in enumerate(decade_list,1):      \n",
    "        df_X, df_Y = df_s(keep_top=None, decade=decade) # Call the function to generate the two dataframes\n",
    "        x0 = np.zeros(len(vars_dec)) # Starting values \n",
    "    \n",
    "        obj_fun = lambda x: sqr_diff_sum(df_X,x) # The objective function -> sum of squared differences\n",
    "        start = time.time()\n",
    "        \n",
    "        # Use Scipy optimizer to solve the model\n",
    "        print(f'Opmization {j} out of {len(decade_list)}: {decade}')\n",
    "        \n",
    "        result_i = optimize.minimize(obj_fun,x0,\n",
    "                               method='Nelder-Mead',\n",
    "                               options={\"disp\":True, \"maxiter\":50000}, # display the results\n",
    "                               );\n",
    "        \n",
    "        end_time = time.time()-start\n",
    "        print(f'{\" \":9s}Time: {end_time:.4f} seconds')\n",
    "        \n",
    "        # Add the result for each deacde to the result-list\n",
    "        result.append(list(result_i.x)) \n",
    "                      \n",
    "    return result # Returns the result-list\n",
    "\n",
    "# Call the optimize_dec function\n",
    "result_dec = optimizer_dec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ce34441bee4dd0b7a43f5b4d4f5819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Variable', options=('Action', 'Adventure', 'Animation', 'Biography…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## The following code produces a figure with the estimated parameters for each deacde for a chosen variable \n",
    "\n",
    "result_dec_mod = [] # Empty list to storage \n",
    "\n",
    "# The loop changes the order of the result-list so the \n",
    "# estimates are ordered by the variables and subordered by decade \n",
    "# insted of being ordered by decade and subordered by variables \n",
    "for j,var in enumerate(vars_dec):\n",
    "    temp = []\n",
    "    for i,dec in enumerate(decade_list):\n",
    "        temp.append(result_dec[i][j])\n",
    "    \n",
    "    result_dec_mod.append(temp)    \n",
    "    \n",
    "# Defining a figure to plot the estimates \n",
    "def fig(var):\n",
    "    \"\"\" Generates a figure which plots estimated parameters for each decade for one variable \n",
    "\n",
    "    Args: \n",
    "        var (type: string): Should be one of the variables in the X-vector\n",
    "\n",
    "    Returns: \n",
    "        One interactive plot.  \n",
    "\n",
    "    Notice: \n",
    "        The function is generated so that it can be called using widgets.interact()\n",
    "        Thus, it is not intended to be used on its own. \n",
    "        \"\"\"\n",
    "    # Initalize figure\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax1 = fig.add_subplot(1,1,1)\n",
    "    \n",
    "    # Plotting variables for chosen decade\n",
    "    ax1.bar(decade_list, result_dec_mod[vars_dec.index(var)],width=6)\n",
    "    \n",
    "    # Setting labels, ticks etc. \n",
    "    ax1.set_ylabel('Parameter estimates')\n",
    "    ax1.set_title(f'Figure 6: Parameter estimates for {var} per decade')\n",
    "    ax1.set_xticks(decade_list)\n",
    "    ax1.axhline(y=0,color='black',linewidth=1)\n",
    "\n",
    "# Making the figure interactive so the estimates are shown for the chosen variable \n",
    "widgets.interact(fig,\n",
    "    var = widgets.Dropdown(description='Variable', value='Action', options=vars_dec, \n",
    "                ),\n",
    ");    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 6 shows the final robustness check, how the estimates vary when the sample only contains movies from one deacde. Going through the different variables, it is hard to find a generel trend. E.g. the estimates of action movies are negative in some decades and positive in other decades. On the other hand, the estimates of war movies are negative for all but two decades, and duration has positive estimates in all decades. <br>\n",
    "The messy picture could be caused by too small samples whereby the estimates will be very volatile. Another reason could be that the preferences change. By looking at the estimates of drama movies you could make the arguement that the the genre of drama affect the ratings positively for old movies, but the opposite after the 1950s. This could be an indication that people have less preferences for newer drama movies compared to older drama movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on the above results, it appears that duration increases ratings by quite a lot, while Nr. of nominations and Nr. of wins doesn't appear to play a role. Meanwhile, the effect of different genres and decades are quite different. We see that genres like Action and Horror have very little effect on ratings, while the Fantasy movies appear to have a signifiant positive marginal effect on ratings.  \n",
    "\n",
    "Our estimates are not precise though, but this is to be expected. It is very difficult to create a model that predicts ratings like these 100 pct. right (if it is even possible with the given variables). Our robustness checks shows, that estimates are volatile to whether only top movies are included and how many of these are included, as shown in figure 4. This volatility is probably a combination of model misspecification and changing preferences. Additionally, genres could have very different distributions (e.g. the best movies are often action-movies, but there are A LOT of bad action movies as well).\n",
    "\n",
    "A way to deal with this volatility could be to consider another functional form of the model or maybe find other variables that could improve how well the model predicts ratings. \n",
    "\n",
    "We hope you enjoyed our movie model project, and don't think we are completely idiots. Because as our model we conclude that *much to learn, we still have*. \n",
    "\n",
    "![The end](https://media.giphy.com/media/lD76yTC5zxZPG/giphy.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
